# ============================================================================
# PipeLineServices Configuration
# Shared environment variables for Ingestion and Retrieval pipelines
# ============================================================================

# ============================================================================
# Environment Configuration
# ============================================================================
# Set to "development" for local development, "production" for server
# This controls service URLs and connection settings
ENVIRONMENT=development

# Service registry environment (dev/staging/prod)
# This is used by config_loader and service_registry for environment detection
PIPELINE_ENV=dev

# ============================================================================
# Inter-Service Communication
# ============================================================================
INTERNAL_MODE=true

# ============================================================================
# Nebius AI Studio Configuration
# ============================================================================
NEBIUS_API_KEY=eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNDg1MDE1NzgwMDYwNDQwNDIxOCIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTkxNzI2MTA0NywidXVpZCI6IjAxOTlhZjM0LTBjZmMtNzdkNi1hNjQ3LTMwYTg2YWUxYzY3YiIsIm5hbWUiOiJDcmF3bEVuZ2luZVBybyIsImV4cGlyZXNfYXQiOiIyMDMwLTEwLTAzVDEyOjMwOjQ3KzAwMDAifQ.M9LRHA4M-9H6oUimGqVdB88KGCFZUtQOhkYYha3vw9U
NEBIUS_API_URL=https://api.studio.nebius.ai/v1/embeddings

# ============================================================================
# LLM Gateway Configuration
# ============================================================================
# Development: Port 8075 (part of Ingestion services 8070-8079)
# Production: Port 8065 (part of Ingestion services 8060-8069)
LLM_GATEWAY_URL_DEVELOPMENT=http://localhost:8075/v1/chat/completions
LLM_GATEWAY_URL_PRODUCTION=http://localhost:8065/v1/chat/completions
LLM_GATEWAY_API_KEY=internal_service_2025_secret_key_metadata_embeddings

# ============================================================================
# Milvus Vector Database Configuration (Environment-aware)
# ============================================================================
# Development: Access via SSH tunnel (ssh -i ~/reku631_nebius -L 19530:localhost:19530 reku631@89.169.103.3)
#              Milvus runs on production server, accessed via tunnel on localhost:19530
# Production: Direct localhost access (Milvus runs locally on server)
MILVUS_HOST_DEVELOPMENT=localhost
MILVUS_PORT_DEVELOPMENT=19530
MILVUS_HOST_PRODUCTION=localhost
MILVUS_PORT_PRODUCTION=19530
MILVUS_USER=
MILVUS_PASSWORD=

# SSH Tunnel Configuration (for documentation - manual setup required)
# Development requires SSH tunnel: ssh -i ~/reku631_nebius -L 19530:localhost:19530 -L 3000:localhost:3000 reku631@89.169.103.3
# This tunnels both Milvus (19530) and Attu UI (3000) from production server to local machine

# ============================================================================
# Ingestion Pipeline Service Ports (DEVELOPMENT: 8070-8079)
# ============================================================================
INGESTION_API_PORT=8070           # Main orchestrator API (public)
CHUNKING_SERVICE_PORT=8071        # Internal only
METADATA_SERVICE_PORT=8072        # Internal only
EMBEDDINGS_SERVICE_PORT=8073      # Internal only
STORAGE_SERVICE_PORT=8074         # Internal only
LLM_GATEWAY_SERVICE_PORT=8075     # LLM Gateway (internal only)

# ============================================================================
# Ingestion Pipeline Service URLs (Internal Mode)
# ============================================================================
CHUNKING_SERVICE_URL=http://localhost:8071/v1/orchestrate
METADATA_SERVICE_URL=http://localhost:8072/v1/metadata
EMBEDDINGS_SERVICE_URL=http://localhost:8073/v1/embeddings
STORAGE_SERVICE_URL=http://localhost:8074/v1
MILVUS_STORAGE_SERVICE_URL=http://localhost:8074/v1

# ============================================================================
# Retrieval Pipeline Service Ports (DEVELOPMENT: 8090-8099)
# ============================================================================
RETRIEVAL_API_PORT=8090           # Main orchestrator API (public)
SEARCH_SERVICE_PORT=8091          # Internal only
RERANK_SERVICE_PORT=8092          # Internal only
COMPRESS_SERVICE_PORT=8093        # Internal only
ANSWER_SERVICE_PORT=8094          # Internal only
INTENT_SERVICE_PORT=8095          # Internal only

# ============================================================================
# Retrieval Pipeline Service URLs (Internal Mode)
# ============================================================================
SEARCH_SERVICE_URL=http://localhost:8091/v1
RERANK_SERVICE_URL=http://localhost:8092/v1
COMPRESS_SERVICE_URL=http://localhost:8093/v1
ANSWER_SERVICE_URL=http://localhost:8094/v1
INTENT_SERVICE_URL=http://localhost:8095/v1

# ============================================================================
# Performance Configuration
# ============================================================================
MAX_WORKERS=5
SERVICE_TIMEOUT=60

# Connection Pooling
CONNECTION_POOL_SIZE=20
CONNECTION_POOL_MAX=100
CONNECTION_TIMEOUT=60

# Batch Processing
BATCH_SIZE=32
MAX_BATCH_SIZE=128
MAX_CONCURRENT_REQUESTS=20
DEFAULT_TIMEOUT=30

# ============================================================================
# Caching Configuration
# ============================================================================
# Global cache control - Set to "false" to disable ALL caching across services
# Services with cache: LLM Gateway, Answer Generation, (Intent has no cache)
ENABLE_CACHE=false                # Master switch: true/false (DISABLED FOR TESTING)
ENABLE_CACHING=false              # Legacy (kept for compatibility)
CACHE_TTL=7200                    # 2 hours (7200 seconds)
CACHE_MAX_SIZE=10000              # 10000 entries

# ============================================================================
# APISIX Gateway Configuration (Legacy - Not Used in New Architecture)
# ============================================================================
APISIX_GATEWAY_URL=http://localhost:9080
APISIX_ADMIN_URL=http://localhost:9180
APISIX_ADMIN_KEY=sk-apisix-admin-mindmate247-2025-v2
SERVICE_API_KEY=sk-internal-service-key

# ============================================================================
# Host Configuration
# ============================================================================
HOST=0.0.0.0

RERANKER_BACKEND=jina

# Jina AI Reranker
# Reranking Models
# RERANKING_MODEL=jina-reranker-v2-base-multilingual                # Semantic reranking (default: Jina-v2)

JINA_AI_KEY=jina_a2e5ee1eea2d444d93e4cc954cecedd9jBBf9EAC3x0Avhynva0mbltZd-Hz
JINA_API_KEY=jina_a2e5ee1eea2d444d93e4cc954cecedd9jBBf9EAC3x0Avhynva0mbltZd-Hz #I need to add JINA_API_KEY (the variable name our embeddings service expects) to the .env file:


# ============================================================================
# Central Model Registry Configuration
# ============================================================================
# ✅ NEW WORKFLOW: All model selection is now managed in shared/model_registry.py
#
# To switch between providers (Nebius/SambaNova), change ONE line:
#   ACTIVE_PRESET = ProviderPreset.SAMBANOVA_FAST  # in shared/model_registry.py line 191
#
# Available presets:
#   - ProviderPreset.NEBIUS_FAST      (Qwen 32B + Llama 8B)
#   - ProviderPreset.SAMBANOVA_FAST   (Qwen 32B + Llama 8B) [FREE]
#   - ProviderPreset.SAMBANOVA_BEST   (DeepSeek R1 671B) [Best Quality]
#   - ProviderPreset.NEBIUS_BALANCED  (Qwen 32B + Llama 70B)
#
# ⚠️  DEPRECATED: Individual model overrides below are no longer needed
# These variables are kept for backwards compatibility but are ignored when
# ACTIVE_PRESET is set in model_registry.py
# ============================================================================

# LLM Models (for different tasks) - DEPRECATED, use model_registry.py instead
# LLM_MODEL_INTENT=Qwen3-32B                                         # Intent detection
# LLM_MODEL_ANSWER_SIMPLE=Meta-Llama-3.1-8B-Instruct                 # Simple queries
# LLM_MODEL_ANSWER_COMPLEX=Qwen3-32B                                 # Complex queries
# LLM_MODEL_COMPRESSION=Qwen3-32B                                    # Context compression
# LLM_MODEL_METADATA=Qwen3-32B                                       # Metadata generation

# Embedding Models
# EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct           # Vector embeddings (default: E5-multilingual)

# ============================================================================
# SambaNova AI Configuration
# ============================================================================
SAMBANOVA_API_KEY=9a2acb34-97f8-4f3c-a37c-d11aa5b699dd

# Sambanova has separate endpoints for LLM and Embeddings
# SAMBANOVA_API_URL is for LLM completions (used by LLM Gateway)
# Embeddings service will use the default: https://api.sambanova.ai/v1/embeddings
# SAMBANOVA_API_URL=https://api.sambanova.ai/v1/chat/completions  # LLM completions only

# SambaNova Model Selection (Optional - uncomment to use SambaNova models)
# SAMBANOVA_REASONING_MODEL=DeepSeek-R1-0528                    # Best reasoning model
# SAMBANOVA_TEXT_MODEL=DeepSeek-V3-0324                         # Best text model
# SAMBANOVA_FAST_MODEL=Meta-Llama-3.1-8B-Instruct              # Fastest model

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=DEBUG