#!/usr/bin/env python3
"""
Central Model Registry for PipeLineServices
Single source of truth for all AI models used across Ingestion and Retrieval pipelines

This module provides:
1. Model enums for type safety
2. Model name mappings (friendly name -> full model ID)
3. Model capabilities and use cases
4. Easy updates in one place

Usage:
    from shared.model_registry import LLMModels, EmbeddingModels, RerankingModels

    # Get model by friendly name
    model_id = LLMModels.QWEN_32B_FAST.value

    # Get model for specific use case
    model_id = get_llm_for_task("answer_generation", complexity="complex")
"""

import os
import logging
from enum import Enum
from typing import Dict, Optional
from pathlib import Path
from dotenv import load_dotenv

# Configure logging
logger = logging.getLogger(__name__)

# Load .env from PipeLineServices root
env_path = Path(__file__).resolve().parents[1] / ".env"
load_dotenv(env_path)

# ============================================================================
# LLM Models (for answer generation, compression, intent detection)
# ============================================================================

class LLMModels(str, Enum):
    """
    Large Language Models for text generation tasks
    Sources: Nebius AI Studio, SambaNova AI
    """
    # ========== NEBIUS AI STUDIO ==========
    # Fast small models (good for simple queries)
    GEMMA_2B = "google/gemma-2-2b-it"                                    # 2B, 8K context, fastest
    GEMMA_9B_FAST = "google/gemma-2-9b-it-fast"                         # 9B, 8K context, very fast
    LLAMA_8B_FAST = "meta-llama/Meta-Llama-3.1-8B-Instruct-fast"       # 8B, 128K context, JSON mode, FASTEST

    # Medium models (balanced speed and reasoning)
    DEVSTRAL_SMALL = "mistralai/Devstral-Small-2505"                    # Small, 128K context, JSON mode, code
    QWEN_32B_FAST = "Qwen/Qwen3-32B-fast"                               # 32B, 41K context, reasoning, code, math, JSON
    QWQ_32B_FAST = "Qwen/QwQ-32B-fast"                                  # 32B, 32K context, reasoning, code, math

    # Large models (best reasoning, slower)
    LLAMA_70B_FAST = "meta-llama/Llama-3.3-70B-Instruct-fast"          # 70B, 128K context
    QWEN_72B = "Qwen/Qwen2.5-72B-Instruct"                              # 72B, balanced model
    QWEN_CODER_480B = "Qwen/Qwen3-Coder-480B-A35B-Instruct"            # 480B, 32K context, code expert
    DEEPSEEK_R1 = "deepseek-ai/DeepSeek-R1-0528"                        # Reasoning model, 64K context
    QWEN_VL_72B = "Qwen/Qwen2.5-VL-72B-Instruct"                        # 72B, 32K context, vision

    # Legacy/base models (non-fast flavors)
    LLAMA_8B = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    LLAMA_70B = "meta-llama/Llama-3.3-70B-Instruct"
    QWEN_32B = "Qwen/Qwen3-32B-Instruct"
    MISTRAL_NEMO = "mistralai/Mistral-Nemo-Instruct-2407"

    # ========== SAMBANOVA AI ==========
    # Reasoning models (with <think> tags - require cleaning)
    SAMBANOVA_DEEPSEEK_R1 = "DeepSeek-R1-0528"                          # 671B MoE, best reasoning, 64K context
    SAMBANOVA_DEEPSEEK_R1_DISTILL_LLAMA_70B = "DeepSeek-R1-Distill-Llama-70B"  # 70B distilled, fast reasoning
    # SAMBANOVA_DEEPSEEK_R1_DISTILL_QWEN_32B = "DeepSeek-R1-Distill-Qwen-32B"    # ‚ùå NOT AVAILABLE on SambaNova (returns "Model not found")
    SAMBANOVA_DEEPSEEK_V3_TERMINUS = "DeepSeek-V3.1-Terminus"           # V3.1, advanced reasoning
    SAMBANOVA_QWEN_32B = "Qwen3-32B"                                    # 32B, fast reasoning
    SAMBANOVA_GPT_OSS_120B = "gpt-oss-120b"                             # 120B, general purpose

    # Text generation models (standard)
    SAMBANOVA_DEEPSEEK_V3 = "DeepSeek-V3-0324"                          # 671B MoE, best quality, 64K context
    SAMBANOVA_LLAMA_SWALLOW_70B = "Llama-3.3-Swallow-70B-Instruct-v0.4"  # 70B, Japanese-optimized
    SAMBANOVA_LLAMA_8B = "Meta-Llama-3.1-8B-Instruct"                   # 8B, fastest, 128K context
    SAMBANOVA_LLAMA_70B = "Meta-Llama-3.3-70B-Instruct"                 # 70B, high quality, 128K context

# ============================================================================
# Embedding Models (for vector search)
# ============================================================================

class EmbeddingModels(str, Enum):
    """
    Embedding models for vector representations
    Sources: Nebius AI Studio, SambaNova AI, Jina AI
    """
    # Nebius embeddings (tested dimensions from Nebius API - 2025-10-10)
    E5_MISTRAL = "intfloat/e5-mistral-7b-instruct"  # 4096 dims (tested), best for RAG
    BGE_EN_ICL = "BAAI/bge-en-icl"  # 4096 dims (tested), English only
    BGE_MULTILINGUAL = "BAAI/bge-multilingual-gemma2"  # 3584 dims (tested), multilingual
    QWEN3_EMBEDDING = "Qwen/Qwen3-Embedding-8B"  # 4096 dims (tested), NOT 1024 as documented

    # SambaNova AI embeddings (FREE tier)
    SAMBANOVA_E5_MISTRAL = "E5-Mistral-7B-Instruct"  # 4096 dims, 4K context, FREE

    # Jina AI embeddings (true 1024-dim models for performance)
    JINA_EMBEDDINGS_V3 = "jina-embeddings-v3"  # 1024 dims, 89 languages, 8K context
    JINA_EMBEDDINGS_V4 = "jina-embeddings-v4"  # 2048 dims, multimodal, latest

    # Alternative embedding models (if needed in future)
    # OPENAI_EMBED_3_SMALL = "text-embedding-3-small"
    # OPENAI_EMBED_3_LARGE = "text-embedding-3-large"

# ============================================================================
# Reranking Models (for semantic reranking)
# ============================================================================

class RerankingModels(str, Enum):
    """
    Reranking models for semantic relevance scoring
    Source: Jina AI, Cohere, BGE
    """
    # BGE rerankers (current default for local deployment)
    BGE_RERANKER_V2_M3 = "BAAI/bge-reranker-v2-m3"                      # Best for local deployment

    # Jina AI rerankers (API-based)
    JINA_RERANKER_V2 = "jina-reranker-v2-base-multilingual"             # Base model, 90 languages
    JINA_RERANKER_V2_BASE = "jina-reranker-v2-base-multilingual"       # Same as above (alias)
    JINA_RERANKER_V2_TURBO = "jina-reranker-v2-base-multilingual"      # Same as base (turbo deprecated)

    # Future options
    # COHERE_RERANK_3 = "rerank-english-v3.0"
    # COHERE_RERANK_MULTILINGUAL = "rerank-multilingual-v3.0"

# ============================================================================
# Provider Presets - Single Source of Truth for Model Selection
# ============================================================================

class ProviderPreset:
    """
    Predefined provider configurations for easy switching.

    CHANGE ONLY ONE LINE (ACTIVE_PRESET) TO SWITCH ALL MODELS!
    """

    NEBIUS_FAST = {
        "intent": LLMModels.QWEN_32B_FAST.value,              # Qwen/Qwen3-32B-fast
        "answer_simple": LLMModels.LLAMA_8B_FAST.value,       # meta-llama/Meta-Llama-3.1-8B-Instruct-fast
        "answer_complex": LLMModels.QWEN_32B_FAST.value,      # Qwen/Qwen3-32B-fast
        "compression": LLMModels.QWEN_32B_FAST.value,         # Qwen/Qwen3-32B-fast
        "metadata": LLMModels.QWEN_32B_FAST.value,            # Qwen/Qwen3-32B-fast
        "metadata_enum": "32B-fast",                           # Metadata service enum format
        "embedding": EmbeddingModels.JINA_EMBEDDINGS_V3.value, # jina-embeddings-v3
        "reranking": RerankingModels.JINA_RERANKER_V2_BASE.value, # jina-reranker-v2-base-multilingual
        "provider": "nebius",
        "description": "Nebius AI Studio - Fast models (Qwen 32B + Llama 8B)"
    }

    SAMBANOVA_FAST = {
        "intent": LLMModels.SAMBANOVA_QWEN_32B.value,         # Qwen3-32B
        "answer_simple": LLMModels.SAMBANOVA_LLAMA_8B.value,  # Meta-Llama-3.1-8B-Instruct
        "answer_complex": LLMModels.SAMBANOVA_QWEN_32B.value, # Qwen3-32B
        "compression": LLMModels.SAMBANOVA_QWEN_32B.value,    # Qwen3-32B
        "metadata": LLMModels.SAMBANOVA_QWEN_32B.value,       # Qwen3-32B
        "metadata_enum": "32B-fast",                           # Metadata service enum format
        "embedding": EmbeddingModels.JINA_EMBEDDINGS_V3.value, # jina-embeddings-v3
        "reranking": RerankingModels.JINA_RERANKER_V2_BASE.value, # jina-reranker-v2-base-multilingual
        "provider": "sambanova",
        "description": "SambaNova AI - Fast and FREE (Qwen 32B + Llama 8B)"
    }

    SAMBANOVA_BEST = {
        "intent": LLMModels.SAMBANOVA_DEEPSEEK_R1.value,            # DeepSeek-R1-0528 (671B MoE)
        "answer_simple": LLMModels.SAMBANOVA_LLAMA_70B.value,       # Meta-Llama-3.3-70B-Instruct
        "answer_complex": LLMModels.SAMBANOVA_DEEPSEEK_R1.value,    # DeepSeek-R1-0528 (671B MoE)
        "compression": LLMModels.SAMBANOVA_DEEPSEEK_V3.value,       # DeepSeek-V3-0324 (671B MoE)
        "metadata": LLMModels.SAMBANOVA_QWEN_32B.value,             # Qwen3-32B
        "metadata_enum": "32B-fast",                                 # Metadata service enum format
        "embedding": EmbeddingModels.JINA_EMBEDDINGS_V3.value,      # jina-embeddings-v3
        "reranking": RerankingModels.JINA_RERANKER_V2_BASE.value,  # jina-reranker-v2-base-multilingual
        "provider": "sambanova",
        "description": "SambaNova AI - Best Quality (DeepSeek R1 671B)"
    }

    NEBIUS_BALANCED = {
        "intent": LLMModels.QWEN_32B_FAST.value,              # Qwen/Qwen3-32B-fast
        "answer_simple": LLMModels.LLAMA_8B_FAST.value,       # meta-llama/Meta-Llama-3.1-8B-Instruct-fast
        "answer_complex": LLMModels.LLAMA_70B_FAST.value,     # meta-llama/Llama-3.3-70B-Instruct-fast
        "compression": LLMModels.QWEN_32B_FAST.value,         # Qwen/Qwen3-32B-fast
        "metadata": LLMModels.QWEN_32B_FAST.value,            # Qwen/Qwen3-32B-fast
        "metadata_enum": "32B-fast",                           # Metadata service enum format
        "embedding": EmbeddingModels.E5_MISTRAL.value,        # intfloat/e5-mistral-7b-instruct
        "reranking": RerankingModels.JINA_RERANKER_V2_BASE.value, # jina-reranker-v2-base-multilingual
        "provider": "nebius",
        "description": "Nebius AI Studio - Balanced (Llama 70B for complex queries)"
    }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üéØ CHANGE ONLY THIS LINE TO SWITCH PROVIDERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
ACTIVE_PRESET = ProviderPreset.SAMBANOVA_FAST  # ‚Üê ONE LINE CHANGE!
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


# Auto-populate defaults from active preset
DEFAULT_LLM_INTENT = ACTIVE_PRESET["intent"]
DEFAULT_LLM_ANSWER_SIMPLE = ACTIVE_PRESET["answer_simple"]
DEFAULT_LLM_ANSWER_COMPLEX = ACTIVE_PRESET["answer_complex"]
DEFAULT_LLM_COMPRESSION = ACTIVE_PRESET["compression"]
DEFAULT_LLM_METADATA = ACTIVE_PRESET["metadata"]
DEFAULT_METADATA_ENUM = ACTIVE_PRESET["metadata_enum"]
DEFAULT_EMBEDDING_MODEL = ACTIVE_PRESET["embedding"]
DEFAULT_RERANKING_MODEL = ACTIVE_PRESET["reranking"]
ACTIVE_PROVIDER = ACTIVE_PRESET["provider"]

# ============================================================================
# ModelInfo Class - Unified Model Naming (Phase 2)
# ============================================================================

class ModelInfo:
    """
    Unified model information class that contains all name formats.

    Eliminates the need for map_model_to_metadata_enum() mapping function.
    """
    def __init__(
        self,
        canonical: str,
        api_name: str,
        metadata_enum: str,
        provider: str,
        description: str = ""
    ):
        self.canonical = canonical          # Human-readable name: "Qwen 32B Fast"
        self.api_name = api_name           # API request format: "Qwen3-32B"
        self.metadata_enum = metadata_enum # Metadata service enum: "32B-fast"
        self.provider = provider           # Provider: "nebius" | "sambanova"
        self.description = description     # Optional description

# ============================================================================
# Model Registry Dictionary - All Models with All Name Formats
# ============================================================================

MODELS = {
    # Nebius Models
    "nebius_qwen_32b_fast": ModelInfo(
        canonical="Qwen 32B Fast",
        api_name="Qwen/Qwen3-32B-fast",
        metadata_enum="32B-fast",
        provider="nebius",
        description="Qwen 32B fast model on Nebius"
    ),
    "nebius_llama_8b_fast": ModelInfo(
        canonical="Llama 8B Fast",
        api_name="meta-llama/Meta-Llama-3.1-8B-Instruct-fast",
        metadata_enum="7B-fast",
        provider="nebius",
        description="Llama 3.1 8B fast model on Nebius"
    ),
    "nebius_llama_70b": ModelInfo(
        canonical="Llama 70B",
        api_name="meta-llama/Meta-Llama-3.1-70B-Instruct",
        metadata_enum="72B",
        provider="nebius",
        description="Llama 3.1 70B model on Nebius"
    ),

    # SambaNova Models
    "sambanova_qwen_32b": ModelInfo(
        canonical="Qwen 32B",
        api_name="Qwen3-32B",
        metadata_enum="32B-fast",
        provider="sambanova",
        description="Qwen 32B model on SambaNova (FREE)"
    ),
    "sambanova_llama_8b": ModelInfo(
        canonical="Llama 8B",
        api_name="Meta-Llama-3.1-8B-Instruct",
        metadata_enum="7B-fast",
        provider="sambanova",
        description="Llama 3.1 8B model on SambaNova (FREE)"
    ),
    "sambanova_deepseek_r1": ModelInfo(
        canonical="DeepSeek R1 671B",
        api_name="DeepSeek-R1-0528",
        metadata_enum="480B",  # Closest match for large model
        provider="sambanova",
        description="DeepSeek R1 reasoning model on SambaNova (BEST QUALITY)"
    ),
}

# ============================================================================
# Helper Function - Get Metadata Enum from API Name
# ============================================================================

def get_metadata_enum_for_model(api_name: str) -> str:
    """
    Get metadata service enum format for a given API name.

    This replaces map_model_to_metadata_enum() with a simple lookup.

    Args:
        api_name: The API name from LLM model (e.g., "Qwen3-32B")

    Returns:
        Metadata enum format (e.g., "32B-fast")

    Example:
        >>> get_metadata_enum_for_model("Qwen3-32B")
        "32B-fast"
    """
    # Try exact match first
    for model_info in MODELS.values():
        if model_info.api_name == api_name:
            return model_info.metadata_enum

    # Try partial match (case-insensitive)
    api_name_lower = api_name.lower()
    for model_info in MODELS.values():
        if model_info.api_name.lower() in api_name_lower:
            return model_info.metadata_enum

    # Default fallback (should never happen if presets are used correctly)
    logger.warning(f"No metadata enum found for model: {api_name}, defaulting to 32B-fast")
    return "32B-fast"

# Legacy: Allow .env overrides (for backwards compatibility during migration)
# TODO: Remove these after all services are updated
if os.getenv("LLM_MODEL_INTENT"):
    DEFAULT_LLM_INTENT = os.getenv("LLM_MODEL_INTENT")
if os.getenv("LLM_MODEL_ANSWER_SIMPLE"):
    DEFAULT_LLM_ANSWER_SIMPLE = os.getenv("LLM_MODEL_ANSWER_SIMPLE")
if os.getenv("LLM_MODEL_ANSWER_COMPLEX"):
    DEFAULT_LLM_ANSWER_COMPLEX = os.getenv("LLM_MODEL_ANSWER_COMPLEX")
if os.getenv("LLM_MODEL_COMPRESSION"):
    DEFAULT_LLM_COMPRESSION = os.getenv("LLM_MODEL_COMPRESSION")
if os.getenv("LLM_MODEL_METADATA"):
    DEFAULT_LLM_METADATA = os.getenv("LLM_MODEL_METADATA")
if os.getenv("EMBEDDING_MODEL"):
    DEFAULT_EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
if os.getenv("RERANKING_MODEL"):
    DEFAULT_RERANKING_MODEL = os.getenv("RERANKING_MODEL")

# ============================================================================
# Model Capabilities Matrix
# ============================================================================

LLM_CAPABILITIES = {
    # Fast small models - good for simple retrieval
    LLMModels.GEMMA_2B: {
        "params": "2B",
        "context_window": 8192,
        "speed": "fastest",
        "use_cases": ["simple_lookup", "yes_no", "list_enumeration"],
        "supports_json": False,
        "supports_reasoning": False,
        "cost_tier": "ultra_low"
    },
    LLMModels.LLAMA_8B_FAST: {
        "params": "8B",
        "context_window": 131072,  # 128K
        "speed": "very_fast",
        "use_cases": ["simple_lookup", "factual_retrieval", "definition_explanation", "comparison"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "low"
    },
    LLMModels.GEMMA_9B_FAST: {
        "params": "9B",
        "context_window": 8192,
        "speed": "very_fast",
        "use_cases": ["simple_lookup", "factual_retrieval", "list_enumeration"],
        "supports_json": False,
        "supports_reasoning": False,
        "cost_tier": "low"
    },

    # Medium models - balanced speed and reasoning
    LLMModels.DEVSTRAL_SMALL: {
        "params": "~22B",
        "context_window": 131072,  # 128K
        "speed": "fast",
        "use_cases": ["factual_retrieval", "comparison", "code_generation"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "medium"
    },
    LLMModels.QWEN_32B_FAST: {
        "params": "32B",
        "context_window": 41000,  # 41K
        "speed": "medium",
        "use_cases": ["cross_reference", "synthesis", "aggregation", "temporal", "relationship_mapping", "contextual_explanation"],
        "supports_json": True,
        "supports_reasoning": True,  # Has <think> tags - MUST clean with re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL)
        "cost_tier": "medium",
        "recommended_for": "complex_queries",
        "requires_cleaning": True,  # Must strip <think> reasoning tags from output
        "cleaning_pattern": r'<think>.*?</think>'  # Use this regex pattern to clean output
    },
    LLMModels.QWQ_32B_FAST: {
        "params": "32B",
        "context_window": 32768,  # 32K
        "speed": "medium",
        "use_cases": ["aggregation", "temporal", "mathematical_reasoning"],
        "supports_json": True,
        "supports_reasoning": True,  # Has <think> tags - MUST clean with re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL)
        "cost_tier": "medium",
        "requires_cleaning": True,  # Must strip <think> reasoning tags from output
        "cleaning_pattern": r'<think>.*?</think>'  # Use this regex pattern to clean output
    },

    # Large models - best for complex reasoning
    LLMModels.LLAMA_70B_FAST: {
        "params": "70B",
        "context_window": 131072,  # 128K
        "speed": "slow",
        "use_cases": ["synthesis", "complex_reasoning", "multi_step_analysis"],
        "supports_json": False,
        "supports_reasoning": False,
        "cost_tier": "high"
    },
    LLMModels.QWEN_72B: {
        "params": "72B",
        "context_window": 131072,  # 128K
        "speed": "slow",
        "use_cases": ["synthesis", "complex_reasoning", "multi_step_analysis"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "high"
    },
    LLMModels.QWEN_CODER_480B: {
        "params": "480B",
        "context_window": 32768,
        "speed": "very_slow",
        "use_cases": ["code_generation", "code_analysis", "technical_synthesis"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "very_high"
    },
    LLMModels.DEEPSEEK_R1: {
        "params": "~70B",
        "context_window": 65536,  # 64K
        "speed": "slow",
        "use_cases": ["complex_reasoning", "mathematical_proofs", "multi_step_analysis"],
        "supports_json": False,
        "supports_reasoning": True,  # Has <think> tags - MUST clean with re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL)
        "cost_tier": "high",
        "requires_cleaning": True,  # Must strip <think> reasoning tags from output
        "cleaning_pattern": r'<think>.*?</think>',  # Use this regex pattern to clean output
        "provider": "nebius"
    },

    # ========== SAMBANOVA AI MODELS ==========
    # Reasoning models
    LLMModels.SAMBANOVA_DEEPSEEK_R1: {
        "params": "671B MoE",
        "context_window": 65536,  # 64K
        "speed": "slow",
        "use_cases": ["complex_reasoning", "mathematical_proofs", "multi_step_analysis", "scientific_research"],
        "supports_json": True,
        "supports_reasoning": True,  # Has <think> tags
        "cost_tier": "ultra_low",  # SambaNova free tier
        "requires_cleaning": True,
        "cleaning_pattern": r'<think>.*?</think>',
        "provider": "sambanova",
        "recommended_for": "best_reasoning"
    },
    LLMModels.SAMBANOVA_DEEPSEEK_R1_DISTILL_LLAMA_70B: {
        "params": "70B",
        "context_window": 65536,  # 64K
        "speed": "medium",
        "use_cases": ["complex_reasoning", "aggregation", "synthesis"],
        "supports_json": True,
        "supports_reasoning": True,
        "cost_tier": "ultra_low",
        "requires_cleaning": True,
        "cleaning_pattern": r'<think>.*?</think>',
        "provider": "sambanova"
    },
    LLMModels.SAMBANOVA_DEEPSEEK_V3_TERMINUS: {
        "params": "671B MoE",
        "context_window": 65536,  # 64K
        "speed": "slow",
        "use_cases": ["advanced_reasoning", "code_generation", "synthesis"],
        "supports_json": True,
        "supports_reasoning": True,
        "cost_tier": "ultra_low",
        "requires_cleaning": True,
        "cleaning_pattern": r'<think>.*?</think>',
        "provider": "sambanova"
    },
    LLMModels.SAMBANOVA_QWEN_32B: {
        "params": "32B",
        "context_window": 32768,  # 32K
        "speed": "fast",
        "use_cases": ["reasoning", "code", "math"],
        "supports_json": True,
        "supports_reasoning": True,
        "cost_tier": "ultra_low",
        "requires_cleaning": True,
        "cleaning_pattern": r'<think>.*?</think>',
        "provider": "sambanova"
    },
    LLMModels.SAMBANOVA_GPT_OSS_120B: {
        "params": "120B",
        "context_window": 32768,  # 32K
        "speed": "medium",
        "use_cases": ["general_purpose", "factual_retrieval", "synthesis"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "ultra_low",
        "provider": "sambanova"
    },

    # Text generation models
    LLMModels.SAMBANOVA_DEEPSEEK_V3: {
        "params": "671B MoE",
        "context_window": 65536,  # 64K
        "speed": "slow",
        "use_cases": ["synthesis", "complex_writing", "code_generation"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "ultra_low",
        "provider": "sambanova",
        "recommended_for": "best_quality"
    },
    LLMModels.SAMBANOVA_LLAMA_SWALLOW_70B: {
        "params": "70B",
        "context_window": 131072,  # 128K
        "speed": "medium",
        "use_cases": ["multilingual", "japanese", "translation"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "ultra_low",
        "provider": "sambanova"
    },
    LLMModels.SAMBANOVA_LLAMA_8B: {
        "params": "8B",
        "context_window": 131072,  # 128K
        "speed": "very_fast",
        "use_cases": ["simple_lookup", "factual_retrieval", "list_enumeration"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "ultra_low",
        "provider": "sambanova",
        "recommended_for": "fastest"
    },
    LLMModels.SAMBANOVA_LLAMA_70B: {
        "params": "70B",
        "context_window": 131072,  # 128K
        "speed": "medium",
        "use_cases": ["synthesis", "complex_reasoning", "comparison"],
        "supports_json": True,
        "supports_reasoning": False,
        "cost_tier": "ultra_low",
        "provider": "sambanova"
    }
}

# ============================================================================
# Embedding Model Capabilities Matrix
# ============================================================================
#
# ‚úÖ FULLY TESTED - All 3 providers verified with complete ingestion pipeline (2025-10-17)
# Test document: JaiShreeRam.md (22 chunks)
# All collections created successfully in Milvus and viewable in Attu UI
#
# Test Results Summary:
#   1. Jina AI - jina-embeddings-v3 (1024 dims) - ‚úÖ TESTED
#      Collection: jina_v3_test
#      Cost: $0.02/M tokens (most affordable)
#
#   2. Nebius AI - intfloat/e5-mistral-7b-instruct (4096 dims) - ‚úÖ TESTED
#      Collection: nebius_e5_test
#      Cost: Paid (exact pricing TBD)
#
#   3. SambaNova AI - E5-Mistral-7B-Instruct (4096 dims) - ‚úÖ TESTED
#      Collection: sambanova_e5_test
#      Total Time: 15,400.88 ms (15.4 seconds)
#      Breakdown: Chunking 65.59ms | Metadata 5,105.88ms | Embeddings 5,105.88ms | Storage 10,221.16ms
#      Cost: $0.13/M input tokens, $0.00/M output tokens (FREE tier available)
#
# For detailed comparison: /local_dev/EMBEDDING_PROVIDERS_COMPARISON.md
#
# Configuration Note: SambaNova has TWO separate API endpoints:
#   - /v1/chat/completions (for LLM text generation - used by LLM Gateway)
#   - /v1/embeddings (for embeddings - used by Embeddings Service)
#   Do NOT set SAMBANOVA_API_URL in .env for embeddings - let service use default
# ============================================================================

EMBEDDING_CAPABILITIES = {
    # Nebius AI Studio models (tested 2025-10-10)
    EmbeddingModels.E5_MISTRAL: {
        "dimension": 4096,  # Tested: returns 4096 dims
        "context_window": 32768,  # 32K
        "languages": "multilingual",
        "mteb_score": 0.83,  # ~83% on MTEB leaderboard
        "use_case": "Best for RAG/instruction tasks, highest quality",
        "speed": "medium",
        "cost_tier": "standard",
        "provider": "nebius",
        "api_url": "https://api.studio.nebius.ai/v1/embeddings"
    },
    EmbeddingModels.QWEN3_EMBEDDING: {
        "dimension": 4096,  # Tested: returns 4096 dims (NOT 1024 as documented)
        "context_window": 32768,  # 32K
        "languages": "multilingual",
        "mteb_score": 0.79,  # ~79% on MTEB leaderboard
        "use_case": "High performance, balanced quality/speed",
        "speed": "fast",
        "cost_tier": "standard",
        "provider": "nebius",
        "api_url": "https://api.studio.nebius.ai/v1/embeddings"
    },
    EmbeddingModels.BGE_EN_ICL: {
        "dimension": 4096,  # Tested: returns 4096 dims (NOT 1024 as documented)
        "context_window": 32768,  # 32K
        "languages": "english",
        "mteb_score": 0.76,  # ~76% on MTEB leaderboard
        "use_case": "Good for general English retrieval",
        "speed": "fast",
        "cost_tier": "standard",
        "provider": "nebius",
        "api_url": "https://api.studio.nebius.ai/v1/embeddings"
    },
    EmbeddingModels.BGE_MULTILINGUAL: {
        "dimension": 3584,  # Tested: returns 3584 dims (NOT 1024)
        "context_window": 8192,  # 8K
        "languages": "100+ languages",
        "mteb_score": 0.78,  # ~78% on MTEB leaderboard
        "use_case": "Best for multilingual retrieval",
        "speed": "fast",
        "cost_tier": "standard",
        "provider": "nebius",
        "api_url": "https://api.studio.nebius.ai/v1/embeddings"
    },

    # SambaNova AI models (FREE tier) - ‚úÖ FULLY TESTED 2025-10-17
    EmbeddingModels.SAMBANOVA_E5_MISTRAL: {
        "dimension": 4096,  # Same as Nebius E5-Mistral
        "context_window": 4096,  # 4K tokens (per SambaNova docs)
        "languages": "multilingual",
        "mteb_score": 0.83,  # ~83% on MTEB leaderboard (same model as Nebius)
        "use_case": "Best for RAG, same quality as Nebius E5-Mistral but FREE",
        "speed": "medium",
        "cost_tier": "free",  # SambaNova free tier - $0.13/M input, $0.00/M output
        "provider": "sambanova",
        "api_url": "https://api.sambanova.ai/v1/embeddings",
        "free_tokens": "unlimited",  # SambaNova free tier
        "rate_limit": "TBD",  # Check SambaNova rate limits
        "tested": True,
        "test_date": "2025-10-17",
        "test_collection": "sambanova_e5_test",
        "test_timing_ms": 15400.88,  # Total time for 22 chunks
        "test_chunks": 22
    },

    # Jina AI models (true 1024-dim for performance)
    EmbeddingModels.JINA_EMBEDDINGS_V3: {
        "dimension": 1024,  # True 1024 dims with Matryoshka learning
        "context_window": 8192,  # 8K tokens
        "languages": "89 languages",
        "mteb_score": 0.80,  # ~80% on MTEB leaderboard
        "use_case": "High performance 1024-dim, multilingual, 4x faster search than 4096",
        "speed": "very_fast",
        "cost_tier": "free_tier_available",
        "provider": "jina",
        "api_url": "https://api.jina.ai/v1/embeddings",
        "free_tokens": 10_000_000,  # 10M tokens free
        "rate_limit_free": "500 RPM",
        "rate_limit_paid": "2000 RPM"
    },
    EmbeddingModels.JINA_EMBEDDINGS_V4: {
        "dimension": 2048,  # Latest multimodal model
        "context_window": 8192,  # 8K tokens
        "languages": "89 languages + multimodal",
        "mteb_score": 0.82,  # ~82% on MTEB leaderboard
        "use_case": "Multimodal embeddings (text + images), latest model",
        "speed": "fast",
        "cost_tier": "paid",
        "provider": "jina",
        "api_url": "https://api.jina.ai/v1/embeddings"
    }
}

# ============================================================================
# Intent-to-Model Mapping (Dynamic Model Selection)
# ============================================================================

# ============================================================================
# Intent-to-Complexity Mapping (determines simple vs complex model)
# ============================================================================
# Complex intents that need strong reasoning (use answer_complex from preset)
COMPLEX_INTENTS = {
    "cross_reference",      # Needs systematic comparison across sets
    "synthesis",            # Needs multi-source integration and analysis
    "negative_logic",       # Needs NOT/absence detection logic
    "relationship_mapping", # Needs entity relationship graph understanding
    "aggregation",          # Needs mathematical operations and calculations
    "temporal",             # Needs date arithmetic and timeline reasoning
    "contextual_explanation",  # Needs deep reasoning about "why" questions
    "exception_handling"   # Needs understanding of policy violations
    # NOTE: comparison REMOVED - most comparisons are straightforward (moved to simple)
    # NOTE: definition_explanation REMOVED - use fast model
    # NOTE: factual_retrieval REMOVED - use fast model
}

# Simple intents (use answer_simple from preset)
# Everything not in COMPLEX_INTENTS is considered simple

# ============================================================================
# Helper Functions
# ============================================================================

def get_llm_for_task(task: str, complexity: str = "moderate", intent: str = None) -> str:
    """
    Get the best LLM model for a specific task based on active preset

    Args:
        task: Task type (e.g., "answer_generation", "compression", "intent_detection")
        complexity: Query complexity ("simple", "moderate", "complex")
        intent: Query intent (if available)

    Returns:
        Model ID (full model name) from ACTIVE_PRESET
    """
    if task == "intent_detection":
        return DEFAULT_LLM_INTENT

    elif task == "metadata_generation":
        return DEFAULT_LLM_METADATA

    elif task == "compression":
        return DEFAULT_LLM_COMPRESSION

    elif task == "answer_generation":
        # Use intent-based selection if intent is provided
        if intent and intent in COMPLEX_INTENTS:
            # Complex intent - use complex model from active preset
            return DEFAULT_LLM_ANSWER_COMPLEX
        elif intent:
            # Simple intent - use simple model from active preset
            return DEFAULT_LLM_ANSWER_SIMPLE

        # Fallback to complexity-based selection (when intent not provided)
        if complexity == "simple":
            return DEFAULT_LLM_ANSWER_SIMPLE
        elif complexity == "complex":
            return DEFAULT_LLM_ANSWER_COMPLEX
        else:  # moderate
            return DEFAULT_LLM_ANSWER_SIMPLE  # Default to fast model

    # Default fallback
    return DEFAULT_LLM_ANSWER_COMPLEX

def get_embedding_model() -> str:
    """Get the default embedding model"""
    return DEFAULT_EMBEDDING_MODEL

def get_embedding_dimension(model: str = None) -> int:
    """
    Get embedding dimension for a model

    Args:
        model: Model ID (if None, uses DEFAULT_EMBEDDING_MODEL)

    Returns:
        Dimension size (e.g., 1024, 4096)
    """
    if model is None:
        model = DEFAULT_EMBEDDING_MODEL

    info = get_model_info(model)
    return info.get("dimension", 1024)  # Default to 1024 if not found

def get_reranking_model() -> str:
    """Get the default reranking model"""
    return DEFAULT_RERANKING_MODEL

def get_model_info(model: str) -> Dict:
    """
    Get capabilities and info for a model

    Args:
        model: Model ID or enum value

    Returns:
        Dict with model capabilities
    """
    # Try to find model in LLM capabilities matrix
    for model_enum in LLMModels:
        if model_enum.value == model or model_enum.name == model:
            return LLM_CAPABILITIES.get(model_enum, {})

    # Try to find model in embedding capabilities matrix
    for model_enum in EmbeddingModels:
        if model_enum.value == model or model_enum.name == model:
            return EMBEDDING_CAPABILITIES.get(model_enum, {})

    return {}

def supports_reasoning(model: str) -> bool:
    """Check if model supports reasoning tags (<think>)"""
    info = get_model_info(model)
    return info.get("supports_reasoning", False)

def requires_output_cleaning(model: str) -> bool:
    """
    Check if model output requires cleaning (e.g., <think> tag removal)

    Models with reasoning capabilities (Qwen, DeepSeek) emit <think> tags
    that must be stripped before returning answers to users.

    Args:
        model: Model ID or enum value

    Returns:
        True if output needs cleaning, False otherwise
    """
    info = get_model_info(model)
    return info.get("requires_cleaning", False)

def get_cleaning_pattern(model: str) -> str:
    """
    Get the regex pattern for cleaning model output

    Usage:
        import re
        pattern = get_cleaning_pattern(model)
        cleaned = re.sub(pattern, '', raw_output, flags=re.DOTALL)

    Args:
        model: Model ID or enum value

    Returns:
        Regex pattern string (e.g., r'<think>.*?</think>')
    """
    info = get_model_info(model)
    return info.get("cleaning_pattern", "")

def get_model_provider(model: str) -> str:
    """
    Determine which provider to use for a given model

    Args:
        model: Model ID or enum value

    Returns:
        Provider name: "nebius", "sambanova", "jina", "openai", etc.
    """
    info = get_model_info(model)
    provider = info.get("provider", "nebius")  # Default to Nebius

    # Fallback: detect by model name pattern
    if not provider:
        if model.startswith("DeepSeek-") or model.startswith("Qwen3-") or model.startswith("gpt-oss-"):
            return "sambanova"
        elif "jina-" in model.lower():
            return "jina"
        else:
            return "nebius"

    return provider

def is_sambanova_model(model: str) -> bool:
    """Check if model is from SambaNova"""
    return get_model_provider(model) == "sambanova"

def is_nebius_model(model: str) -> bool:
    """Check if model is from Nebius"""
    return get_model_provider(model) == "nebius"

# ============================================================================
# Model Pricing (Nebius AI Studio - same for all models)
# ============================================================================

NEBIUS_PRICING_PER_MILLION_TOKENS = 0.20  # $0.20 per 1M tokens (input + output)

def estimate_cost(input_tokens: int, output_tokens: int) -> float:
    """
    Estimate cost for a request

    Args:
        input_tokens: Number of input tokens
        output_tokens: Number of output tokens

    Returns:
        Estimated cost in USD
    """
    total_tokens = input_tokens + output_tokens
    return (total_tokens / 1_000_000) * NEBIUS_PRICING_PER_MILLION_TOKENS
