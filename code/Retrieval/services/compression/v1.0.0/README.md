# Compression Service v2.0.0

LLM-powered contextual compression - extract only relevant sentences from retrieved chunks.

## ðŸŽ‰ Deployment Status: READY FOR DEPLOYMENT

- â³ **Service Status**: Pending deployment
- â³ **APISIX Route**: To be configured at `/api/v2/compress/*`
- âœ… **Security Middleware**: Blocks external direct access
- âœ… **API Version**: 2.0.0
- â³ **Production Testing**: Pending

## Features

- **LLM-Powered Compression**: Uses LLM Gateway to extract only relevant sentences
- **Batch Processing**: Compresses all chunks in single LLM call (4-6x faster)
- **Versioned API**: `/v2/compress` with backward compatibility `/v1/compress`
- **Health Monitoring**: `/health`, `/version` endpoints
- **Multi-Model Support**: 7B-fast, 72B, 480B models
- **Token Reduction**: 30-70% cost savings on downstream LLM calls
- **Performance Metrics**: Track compression time and ratio per chunk
- **Security Middleware**: Localhost and internal network only
- **APISIX Integration**: API key authentication + rate limiting

## API Endpoints

### Health & Info
- `GET /health` - Service health check
- `GET /version` - Version information

### Compression
- `POST /v2/compress` - Compress chunks by extracting relevant sentences
- `POST /v1/compress` - Legacy endpoint (backward compatibility)

## Usage Example

### Local Testing (Direct Access)
```bash
curl -X POST http://localhost:9003/v2/compress \
  -H "Content-Type: application/json" \
  -d '{
    "chunks": [
      {
        "id": 0,
        "text": "Lord Hanuman is a Hindu deity. He has immense strength and can fly. The weather today is sunny. He is devoted to Lord Rama.",
        "summary": "Description of Hanuman",
        "keywords": "Hanuman, strength, devotion"
      }
    ],
    "question": "What are Hanuman'\''s powers?",
    "compression_ratio": 0.3,
    "max_tokens_per_chunk": 200,
    "model": "7B-fast"
  }'
```

## Response Format

```json
{
  "compressed_chunks": [
    {
      "id": 0,
      "original_text": "Lord Hanuman is a Hindu deity...",
      "compressed_text": "He has immense strength and can fly. He is devoted to Lord Rama.",
      "original_length": 156,
      "compressed_length": 64,
      "compression_ratio": 0.41,
      "compression_time_ms": 850
    }
  ],
  "total_input_tokens": 39,
  "total_output_tokens": 16,
  "total_compression_time_ms": 850,
  "avg_compression_ratio": 0.41,
  "model_used": "7B-fast",
  "api_version": "2.0.0"
}
```

## Installation

```bash
pip install -r requirements.txt
python compression_api.py
```

## Configuration

Edit `.env` file:
```bash
HOST=0.0.0.0
PORT=9003
LLM_GATEWAY_URL=http://localhost:8000/v1/chat/completions
LLM_GATEWAY_API_KEY=
DEFAULT_COMPRESSION_RATIO=0.3
MAX_TOKENS_PER_CHUNK=200
MAX_CHUNKS_PER_REQUEST=20
COMPRESSION_TIMEOUT=30
DEFAULT_MODEL=7B-fast
```

## Production Deployment

### Target Setup
**Server:** 89.169.103.3
**Location:** `/home/reku631/services/compression_v2.0.0/`
**Port:** 9003
**APISIX Route:** `/api/v2/compress/*`
**API Key:** `sk-enterprise-mindmate247-2025-v2`

### Production Access (APISIX Gateway)
```bash
curl -X POST http://89.169.103.3:9080/api/v2/compress/v2/compress \
  -H "apikey: sk-enterprise-mindmate247-2025-v2" \
  -H "Content-Type: application/json" \
  -d '{
    "chunks": [
      {
        "id": 0,
        "text": "Your long text here...",
        "summary": "Optional summary",
        "keywords": "Optional keywords"
      }
    ],
    "question": "What information do I need?",
    "compression_ratio": 0.3,
    "model": "7B-fast"
  }'
```

### Deployment Steps

1. **Create tarball**:
```bash
cd /Users/rakesh/Desktop/CrawlEnginePro/nebius_hosting/ai_studio/hosting/services/compression
tar -czf compression_v2.0.0.tar.gz v2.0.0/
```

2. **Upload to server**:
```bash
scp compression_v2.0.0.tar.gz reku631@89.169.103.3:/home/reku631/
```

3. **Deploy on server**:
```bash
ssh reku631@89.169.103.3
cd /home/reku631/
mkdir -p services/compression_v2.0.0
tar -xzf compression_v2.0.0.tar.gz -C services/compression_v2.0.0 --strip-components=1
cd services/compression_v2.0.0
cp .env.example .env
pip install -r requirements.txt
nohup python compression_api.py > compression.log 2>&1 &
```

4. **Configure APISIX route** (via APISIX Admin API)

5. **Test with and without API keys**

## Security

- **Security Middleware**: Blocks all external direct access
- **APISIX Gateway**: Required for external access
- **API Key Authentication**: Enforced by APISIX
- **Rate Limiting**: Configured at gateway level
- **Internal Only**: Service only accepts localhost/internal network IPs

## Model Selection

| Model | Speed | Quality | Use Case |
|-------|-------|---------|----------|
| **7B-fast** | ~0.8s/batch | Good | High-throughput, real-time |
| **72B** | ~3s/batch | Better | Balanced quality & speed |
| **480B** | ~1.5s/batch | Best | Highest accuracy |

## Benefits

- **Reduce Token Costs**: Save 30-70% on downstream LLM calls
- **Improve Accuracy**: Remove irrelevant information
- **Maintain Context**: Keep exact wording from original text
- **Faster Inference**: Shorter context windows

## Troubleshooting

### Service Health Check
```bash
curl http://localhost:9003/health
```

### Version Check
```bash
curl http://localhost:9003/version
```

### Check Logs
```bash
tail -f /home/reku631/services/compression_v2.0.0/compression.log
```

### Kill Process
```bash
pkill -f compression_api.py
```

## Problems Fixed During Development

1. **Security Middleware**: Added middleware to block external direct access
2. **Versioned Endpoints**: Created `/v2/compress` with `/v1/compress` backward compatibility
3. **Proper Structure**: Organized into v2.0.0 directory with config, models, and API separation
4. **LLM Gateway Integration**: Uses centralized LLM Gateway instead of direct LLM access
5. **Batch Processing**: Compresses all chunks in single call for 4-6x speedup

See `/Users/rakesh/Desktop/CrawlEnginePro/nebius_hosting/ai_studio/hosting/services/ARCHITECTURE.md` for complete architecture.
